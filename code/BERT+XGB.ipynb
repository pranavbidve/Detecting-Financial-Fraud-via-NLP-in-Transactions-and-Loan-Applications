{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ada03ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>fraud_reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>521585</td>\n",
       "      <td>Expert Opinion:\\n\\nThe insured, a 48-year-old ...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>342868</td>\n",
       "      <td>\"Hey there! So, get this - a 42-year-old dude ...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>687698</td>\n",
       "      <td>\"As an insurance agent handling this claim, I ...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227811</td>\n",
       "      <td>1. **Formal Investigation Style:**\\n\\nOn Janua...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>367455</td>\n",
       "      <td>\"Well, well, well, looks like we've got a case...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text fraud_reported\n",
       "0  521585  Expert Opinion:\\n\\nThe insured, a 48-year-old ...              Y\n",
       "1  342868  \"Hey there! So, get this - a 42-year-old dude ...              Y\n",
       "2  687698  \"As an insurance agent handling this claim, I ...              N\n",
       "3  227811  1. **Formal Investigation Style:**\\n\\nOn Janua...              Y\n",
       "4  367455  \"Well, well, well, looks like we've got a case...              N"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('insurance_claims_nlp_gpt_varied.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "998ced5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aadarsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aadarsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aadarsh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "Tokenizing and processing...\n",
      "Text preprocessing completed!\n",
      "\n",
      "Original text sample:\n",
      "Expert Opinion:\n",
      "\n",
      "The insured, a 48-year-old male with a medical doctorate and a penchant for repairing crafts, experienced a significant single-vehicle collision on January 25, 2015. The incident occu...\n",
      "\n",
      "Cleaned text sample:\n",
      "expert opinion the insured a yearold male with a medical doctorate and a penchant for repairing crafts experienced a significant singlevehicle collision on january the incident occurred in columbus so...\n",
      "\n",
      "Tokenized text sample (first 20 tokens):\n",
      "['expert', 'opinion', 'insur', 'yearold', 'male', 'medic', 'doctor', 'penchant', 'repair', 'craft', 'experienc', 'signific', 'singlevehicl', 'collis', 'januari', 'incid', 'occur', 'columbu', 'south', 'carolina']\n",
      "\n",
      "Processed text sample:\n",
      "expert opinion insur yearold male medic doctor penchant repair craft experienc signific singlevehicl collis januari incid occur columbu south carolina drive around oclock morn collis type classifi sid...\n",
      "Text preprocessing completed!\n",
      "\n",
      "Original text sample:\n",
      "Expert Opinion:\n",
      "\n",
      "The insured, a 48-year-old male with a medical doctorate and a penchant for repairing crafts, experienced a significant single-vehicle collision on January 25, 2015. The incident occu...\n",
      "\n",
      "Cleaned text sample:\n",
      "expert opinion the insured a yearold male with a medical doctorate and a penchant for repairing crafts experienced a significant singlevehicle collision on january the incident occurred in columbus so...\n",
      "\n",
      "Tokenized text sample (first 20 tokens):\n",
      "['expert', 'opinion', 'insur', 'yearold', 'male', 'medic', 'doctor', 'penchant', 'repair', 'craft', 'experienc', 'signific', 'singlevehicl', 'collis', 'januari', 'incid', 'occur', 'columbu', 'south', 'carolina']\n",
      "\n",
      "Processed text sample:\n",
      "expert opinion insur yearold male medic doctor penchant repair craft experienc signific singlevehicl collis januari incid occur columbu south carolina drive around oclock morn collis type classifi sid...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_and_process(text):\n",
    "    \"\"\"\n",
    "    Tokenize text, remove stopwords, and apply stemming\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and apply stemming\n",
    "    processed_tokens = [\n",
    "        stemmer.stem(token) for token in tokens \n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# Apply text preprocessing to the 'text' column\n",
    "print(\"Cleaning text...\")\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Tokenizing and processing...\")\n",
    "df['tokenized_text'] = df['cleaned_text'].apply(tokenize_and_process)\n",
    "\n",
    "# Create a processed text column (tokens joined back into strings)\n",
    "df['processed_text'] = df['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "print(f\"\\nOriginal text sample:\")\n",
    "print(df['text'].iloc[0][:200] + \"...\")\n",
    "print(f\"\\nCleaned text sample:\")\n",
    "print(df['cleaned_text'].iloc[0][:200] + \"...\")\n",
    "print(f\"\\nTokenized text sample (first 20 tokens):\")\n",
    "print(df['tokenized_text'].iloc[0][:20])\n",
    "print(f\"\\nProcessed text sample:\")\n",
    "print(df['processed_text'].iloc[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9238e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing Statistics:\n",
      "========================================\n",
      "Original text - Average length: 1076.9 characters\n",
      "Original text - Max length: 1587 characters\n",
      "Original text - Min length: 32 characters\n",
      "\n",
      "Processed text - Average length: 588.8 characters\n",
      "Processed text - Max length: 938 characters\n",
      "Processed text - Min length: 12 characters\n",
      "\n",
      "Tokens per document - Average: 91.5\n",
      "Tokens per document - Max: 135\n",
      "Tokens per document - Min: 2\n",
      "\n",
      "Vocabulary size: 2219 unique tokens\n",
      "Total tokens: 91549\n",
      "\n",
      "Dataframe shape: (1000, 6)\n",
      "New columns added: ['cleaned_text', 'tokenized_text', 'processed_text']\n",
      "\n",
      "Sample processed data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>fraud_reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Expert Opinion:\\n\\nThe insured, a 48-year-old ...</td>\n",
       "      <td>expert opinion insur yearold male medic doctor...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Hey there! So, get this - a 42-year-old dude ...</td>\n",
       "      <td>hey get yearold dude who custom month merced n...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Expert Opinion:\\n\\nThe insured, a 48-year-old ...   \n",
       "1  \"Hey there! So, get this - a 42-year-old dude ...   \n",
       "\n",
       "                                      processed_text fraud_reported  \n",
       "0  expert opinion insur yearold male medic doctor...              Y  \n",
       "1  hey get yearold dude who custom month merced n...              Y  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display statistics about the preprocessed text\n",
    "print(\"Text Preprocessing Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Original text statistics\n",
    "original_lengths = df['text'].str.len()\n",
    "print(f\"Original text - Average length: {original_lengths.mean():.1f} characters\")\n",
    "print(f\"Original text - Max length: {original_lengths.max()} characters\")\n",
    "print(f\"Original text - Min length: {original_lengths.min()} characters\")\n",
    "\n",
    "# Processed text statistics\n",
    "processed_lengths = df['processed_text'].str.len()\n",
    "print(f\"\\nProcessed text - Average length: {processed_lengths.mean():.1f} characters\")\n",
    "print(f\"Processed text - Max length: {processed_lengths.max()} characters\")\n",
    "print(f\"Processed text - Min length: {processed_lengths.min()} characters\")\n",
    "\n",
    "# Token statistics\n",
    "token_counts = df['tokenized_text'].apply(len)\n",
    "print(f\"\\nTokens per document - Average: {token_counts.mean():.1f}\")\n",
    "print(f\"Tokens per document - Max: {token_counts.max()}\")\n",
    "print(f\"Tokens per document - Min: {token_counts.min()}\")\n",
    "\n",
    "# Vocabulary size\n",
    "all_tokens = [token for tokens in df['tokenized_text'] for token in tokens]\n",
    "unique_tokens = set(all_tokens)\n",
    "print(f\"\\nVocabulary size: {len(unique_tokens)} unique tokens\")\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "\n",
    "# Display the dataframe columns\n",
    "print(f\"\\nDataframe shape: {df.shape}\")\n",
    "print(f\"New columns added: {[col for col in df.columns if col not in ['id', 'text', 'fraud_reported']]}\")\n",
    "\n",
    "# Show a sample of the processed data\n",
    "print(f\"\\nSample processed data:\")\n",
    "df[['text', 'processed_text', 'fraud_reported']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa64bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "embeddings = model.encode(df['processed_text'].tolist(), convert_to_tensor=False)\n",
    "\n",
    "df['bert_embeddings'] = [row for row in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335ec98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c26a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
